---
layout: post
title:  "Normalisation par Lots (ou Batch Normalization)"
date:   2021-04-05 15:37:00 +0200
categories: Apprentissage-profond
---




Il est surprenant de constater le manque de contenu en franÃ§ais que lâ€™on peut trouver sur internet Ã  propos de ce concept, pourtant largement utilisÃ© en apprentissage profond. Le foisonnement dâ€™articles traitant du sujet nâ€™est dâ€™ailleurs par toujours Ã©clairant ; beaucoup sâ€™appuyant sur les premiÃ¨res explications proposÃ©es par les inventeurs de la technique, qui ont Ã©tÃ© trÃ¨s largement remises en question depuis la publication de lâ€™article original.


Objectifs de cet article : 
- Permettre dâ€™apprÃ©hender le concept de Normalisation par lots selon 3 niveaux de complexitÃ© :  en 30 secondes, en 3 minutes, et dans une exploration plus dÃ©taillÃ©e ;
- Aborder les Ã©lÃ©ments clefs Ã  avoir Ã  lâ€™esprit pour exploiter efficacement la couche BN ;
- Proposer une implÃ©mentation simple de la couche BN sous PyTorch, pour voir en dÃ©tail sa mise en pratique ;
- Faire le point sur le niveau de comprÃ©hension actuel que lâ€™on a de ce concept.





| Nom franÃ§ais          | Nom anglais         | AbrÃ©viation courante |
|-----------------------|---------------------|----------------------|
| Normalization par lot | Batch Normalization | BN                   |






## A) En 30 secondes


La **Normalisation par lots** (en anglais ***Batch-Normalization*** - notÃ©e ***BN***) est une mÃ©thode algorithmique qui permet dâ€™entraÃ®ner un rÃ©seau de neurones profond de maniÃ¨re plus rapide et plus stable. 

Cette mÃ©thode consiste Ã  normaliser les vecteurs dâ€™activation des couches cachÃ©es en utilisant les caractÃ©ristiques statistiques du lot (ou *batch*) - la moyenne et lâ€™Ã©cart-type - juste avant (ou juste aprÃ¨s) le passage dans la fonction non-linÃ©aire.



<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_1a_fr.jpg">
  SchÃ©ma 1.a Perceptron multicouche <strong>sans normalisation par lots (BN)</strong>
</p>

	
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_1b_fr.jpg">
  SchÃ©ma 1.b Perceptron multicouche <strong>avec normalisation par lots (BN)</strong>
</p>


Toutes les infrastructures de dÃ©veloppements (ou frameworks) populaires proposent des implÃ©mentations de cette mÃ©thode sous la forme de couche computationnelle, que lâ€™on peut facilement insÃ©rer dans un rÃ©seau de neurones.




<ins>Article de rÃ©fÃ©rence :</ins> [â€œBatch-normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftâ€](url=https://arxiv.org/abs/1502.03167) (trad. â€œNormalisation par Lots : AccÃ©lÃ©ration de lâ€™entraÃ®nement des rÃ©seaux de neurones profonds par la rÃ©duction du dÃ©calage de covariable interneâ€).

<ins>Article (contribution significative dans la comprÃ©hension du concept) :</ins> [â€œHow does batch normalization help optimizationâ€](url=https://arxiv.org/pdf/1805.11604.pdf) (trad. â€œComment la normalisation par lots facilite lâ€™optimisation.â€).


## B) En 3 minutes

### 1) Principe

La normalisation par lot sâ€™articule diffÃ©remment pendant la phase dâ€™entraÃ®nement et la phase dâ€™Ã©valuation.

#### a) Phase dâ€™entraÃ®nement

Pour chaque couche cachÃ©e, on calcule la normalisation par lot de la faÃ§on suivante :

AJOUTER LES EQUATIONS !!!!!!!!!!!!!!
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/bn_eq1.gif">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/bn_eq2.gif">
</p>

<p align="center">
  <img src="https://latex.codecogs.com/svg.image?\mu&space;=&space;\frac{1}{n}*\sum_{i}Z^{(i)}" title="\mu = \frac{1}{n}*\sum_{i}Z^{(i)}" />
</p>


- On calcule dâ€™abord la moyenne ğœ‡ et lâ€™Ã©cart-type Ïƒ des vecteurs dâ€™activations Ã  lâ€™Ã©chelle du lot (1) et (2).
- En utilisant ces valeurs, on normalise le vecteur dâ€™activation Z(i) (3). De cette faÃ§on, la distribution des valeurs dâ€™activations associÃ©es Ã  chaque exemple du lot suit une loi normale centrÃ©e rÃ©duite. (ğœ€ est ici une constante de stabilisation numÃ©rique)

AJOUTER SCHEMA 2
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_2.jpg">
  <strong>SchÃ©ma 2 : 1Ã¨re Ã©tape de la normalisation par lots.</strong> Exemple dâ€™une couche de 3 neurones, avec un lot de taille b. Pour chaque neurone, les valeurs Ã  lâ€™Ã©chelles du batch suivent une loi normal centrÃ©e rÃ©duite.
</p>


Finalement, on calcule les valeurs de **sortie de la couche de normalisation par lot** áº(i) en appliquant une transformation linÃ©aire avec deux paramÃ¨tres Ã  entraÃ®ner (4). Cette derniÃ¨re opÃ©ration permet au modÃ¨le de dÃ©finir Ã  chaque couche cachÃ©e la distribution optimale, en ajustant ces deux paramÃ¨tres :
- ğ›¾ permet de jouer sur lâ€™Ã©talement de la gaussienne ;
- ğ›½ joue le rÃ´le de biais, dÃ©calant Ã  gauche ou Ã  droite la gaussienne.


AJOUTER SCHEMA 3
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_3.jpg">
  <strong>SchÃ©ma 3: IntÃ©rÃªt des paramÃ¨tres ğ›¾ et ğ›½.</strong> Les modifications sur la distribution (en haut) permettent dâ€™exploiter diffÃ©rentes parties de la fonction non-linÃ©aire (en bas).
</p>

<ins>Remarque :</ins> Les raisons qui rendent la couche BN efficace ont souvent fait lâ€™objet dâ€™incomprÃ©hensions et dâ€™erreurs, jusque dans lâ€™article officiel. Des recherches rÃ©centes ont Ã©cartÃ©es certaines hypothÃ¨ses erronÃ©es, et ont permis une meilleure comprÃ©hension de cette technique. Ces aspects sont abordÃ©s plus largement dans la partie C.III : â€œPourquoi la couche BN est-elle efficace ?â€ de cet article.


Ã€ chaque itÃ©ration, le rÃ©seau calcule la moyenne ğœ‡ et lâ€™Ã©cart-type Ïƒ correspondant au lot en cours. Les paramÃ¨tres ğ›¾ et ğ›½ sont ajustÃ©s via la rÃ©tropropagation des gradients, en appliquant une [moyenne mobile](https://fr.wikipedia.org/wiki/Moyenne_mobile). De cette faÃ§on, lâ€™ajustement des paramÃ¨tres ğ›¾ et ğ›½ tiennent davantage compte des derniÃ¨res itÃ©rations que des premiÃ¨res. 

#### b) Phase dâ€™Ã©valuation

Contrairement Ã  la phase dâ€™entraÃ®nement, **on ne dispose pas forcÃ©ment dâ€™un lot complet Ã  infÃ©rer lors de lâ€™Ã©valuation.**

Pour sâ€™affranchir de ce problÃ¨me, on dÃ©termine (ğœ‡pop , Ïƒpop), tel que :
- ğœ‡pop : estimation de la moyenne de la population Ã©tudiÃ©e ;
- Ïƒpop : estimation de lâ€™Ã©cart-type de la population Ã©tudiÃ©e.

Ces valeurs sont dÃ©terminÃ©es Ã  partir des (ğœ‡lot , Ïƒlot) rencontrÃ©s pendant l'entraÃ®nement, et appliquÃ©e systÃ©matiquement dans lâ€™Ã©quation (3), au lieu dâ€™avoir recours aux Ã©quations (1) et (2).

<ins>Remarque :</ins> Cet aspect est plus largement dÃ©crit dans la partie C.II.3 : ParamÃ¨tres statistiques lors de la phase dâ€™Ã©valuationâ€.


### 2) Principe

En pratique, on considÃ¨re la normalisation par lots comme une couche Ã  part entiÃ¨re, au mÃªme titre quâ€™un perceptron, quâ€™une couche de convolution, quâ€™une fonction dâ€™activation ou quâ€™un dropout.

On trouve la couche de normalisation par lots (ou couche BN) dans les infrastructures de dÃ©veloppements (ou frameworks) populaires.

| Librairie          | Couches BN
|--------------------|------------------------------------------------------------------|
| Pytorch            | torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d |
| Tensorflow / Keras | tf.nn.batch_normalization, tf.keras.layers.BatchNormalization    |

<ins>Remarque :</ins> Il est trÃ¨s facile de trouver la documentation de la couche BN pour votre infrastructure de dÃ©veloppement, quâ€™il sâ€™agisse de Mxnet, Matlab, Caffe â€¦  


Toutes donnent la possibilitÃ©s de modifier les paramÃ¨tres que cette mÃ©thode fait intervenir ; dans la pratique, **le paramÃ¨tre le plus important est la taille du vecteur dâ€™entrÃ©e**, Ã  savoir :
- Le nombre de neurones de la couche cachÃ©e, dans le cas dâ€™un perceptron multicouche ;
- Le nombre de filtres de la couche cachÃ©e, dans le cas dâ€™un rÃ©seau convolutif.


### 3) Un coup dâ€™oeil aux rÃ©sultats

Si lâ€™on est loin dâ€™avoir compris tous les mÃ©canismes sous-jacents Ã  la couche BN (voir C.III), il y a un point sur lequel tout le monde sâ€™accorde : Ã§a marche.

En guise de mise en bouche, regardons rapidement les rÃ©sultats obtenus dans lâ€™article officiel [1] :

AJOUTER GRAPHIQUE 1
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_1.png">
  <strong>Graphique 1 : EfficacitÃ© de la couche BN en entraÃ®nement</strong> (source : [1]). PrÃ©cision sur le jeu de validation ImageNet (2012) en fonction du nombre dâ€™itÃ©ration d'entraÃ®nement, pour des rÃ©seaux Inception avec ou sans BN, en augmentant les taux dâ€™apprentissage pour les rÃ©seaux BN (1 fois, 5 fois, 30 fois le taux optimal du rÃ©seau Inception).
</p>

Le rÃ©sultat est net : en ajoutant des couches BN, le **rÃ©seau sâ€™entraÃ®ne plus vite et plus efficacement**.


VoilÃ  de quoi comprendre le principe des couches BN, leur intÃ©rÃªt, et dâ€™Ãªtre en mesure de les utiliser en pratique. une comprÃ©hension un peu plus approfondie est cependant nÃ©cessaire pour ne pas tomber des nues devant le comportement dâ€™un rÃ©seau de neurone.


## C) Comprendre la Normalisation par lots (BN)

### I) ImplÃ©mentation

Jâ€™ai rÃ©-implÃ©mentÃ© cette mÃ©thode sous Pytorch, de maniÃ¨re Ã  retrouver les rÃ©sultats de lâ€™article officiel. Vous pourrez le trouver dans [ce repo git](https://github.com/Johann-Huber/batchnorm_pytorch/blob/main/batch_normalization_in_pytorch.ipynb).

Je vous invite Ã  parcourir les diverses implÃ©mentations de la couche BN disponible en ligne (presque toujours en anglais), en premier lieu celle de l'infrastructure avec laquelle vous travaillez.

### II) La couche BN en pratique

#### 1) RÃ©sultats de lâ€™article original

Jâ€™ai dÃ©cidÃ© de commencer par prÃ©senter les rÃ©sultats obtenus avec la couche de normalisation par lots car **câ€™est le point sur lequel tout sâ€™accorde** la concernant : **Elle est efficace en pratique.**

Lâ€™article officiel [1] a rÃ©alisÃ© 3 expÃ©riences pour Ã©valuer lâ€™efficacitÃ© de leur mÃ©thode. 

La premiÃ¨re a pour but de montrer lâ€™efficacitÃ© de la normalisation par lots sur un exemple simple : Il sâ€™agit dâ€™entraÃ®ner un classificateur sur le jeu de donnÃ©e MNIST (reconnaissance de chiffres Ã©crits Ã  la main, issue du cÃ©lÃ¨bre article de Y. Lecun). Le modÃ¨le consiste en une succession de 3 couches entiÃ¨rement connectÃ©es de 100 neurones, suivis de fonctions sigmoÃ¯des. On entraÃ®ne le tout sur 50 000 itÃ©rations en utilisant un algorithme de gradient stochastique (en anglais Stochastic Gradient Descent - notÃ©e SGD), avec ou sans couche de normalisation par lots pour comparer.

Ce rÃ©sultat peut Ãªtre reproduit rapidement sans GPU, je vous invite Ã  essayer par vous-mÃªme pour vous faire la main.

AJOUTER GRAPHIQUE 2
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_2.png">
  <strong></strong> 
</p>


Bonne nouvelle, la normalisation par lots amÃ©liore les performances du rÃ©seau.

Pour la deuxiÃ¨me expÃ©rience, regardons lâ€™impact de cette mÃ©thode sur lâ€™activation des neurones au niveau des couches cachÃ©es. Voici les valeurs dâ€™activations obtenues sur la derniÃ¨re couche cachÃ©e, juste avant le passage dans la fonction non-linÃ©aire :

AJOUTER GRAPHIQUE 3
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_3.png">
  <strong></strong> 
</p>

Sans la normalisation par lot, les valeurs dâ€™activations varient fortement au cours des premiÃ¨res itÃ©rations. En revanche, les courbes dâ€™activations ne prÃ©sentent pas dâ€™Ã -coups avec lâ€™utilisation de couches BN. 


AJOUTER GRAPHIQUE 4
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_4.png">
  <strong></strong> 
</p>


Le signal est dâ€™ailleurs moins bruitÃ©, lorsque lâ€™on utilise la normalisation par lots. On constate que lâ€™optimiseur (en anglais optimizer) fait converger les poids beaucoup plus facilement.

Cet exemple simple ne montre cependant pas toute lâ€™Ã©tendue de lâ€™impact de cette mÃ©thode.

Lâ€™article officiel explore une troisiÃ¨me expÃ©rience. Il sâ€™agit dâ€™Ã©valuer les performances de la couche BN sur un modÃ¨le classificateur plus complexe, appliquÃ© Ã  la base de donnÃ©e ImageNet (2012). Pour cela, les auteurs adaptent un rÃ©seau de neurone trÃ¨s performant (pour lâ€™Ã©poque) intitulÃ© [Inception](https://arxiv.org/abs/1409.4842), en lui ajoutant des couches de normalisation par lot. Ils comparent ensuite des rÃ©sultats du rÃ©seau original avec plusieurs versions modifiÃ©es. 

Ils obtiennent les rÃ©sultats suivant :

AJOUTER GRAPHIQUE 5
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_1.png">
  <strong>Graphique 1 : EfficacitÃ© de la couche BN en entraÃ®nement</strong> (source : [1]). PrÃ©cision sur le jeu de validation ImageNet (2012) en fonction du nombre dâ€™itÃ©ration d'entraÃ®nement, pour des rÃ©seaux Inception avec ou sans BN, en augmentant les taux dâ€™apprentissage pour les rÃ©seaux BN (1 fois, 5 fois, 30 fois le taux optimal du rÃ©seau Inception).
</p>


AJOUTER GRAS ET COULEURS

Avec :
- BN-Baseline : MÃªme rÃ©seau quâ€™Inception, avec des couches de BN
- BN-x5 : MÃªme rÃ©seau quâ€™Inception, avec des couches de BN, et un taux dâ€™apprentissage (learning rate - notÃ© LR) multipliÃ© par 5
- BN-x30 : MÃªme rÃ©seau quâ€™Inception, avec des couches de BN, et un taux dâ€™apprentissage multipliÃ© par 30
- BN-x5-Sigmoid : MÃªme rÃ©seau quâ€™Inception, avec des couches de BN, un taux dâ€™apprentissage multipliÃ© par 5, et des fonction sigmoÃ¯des Ã  la place des ReLU

Voici ce quâ€™on peut conclure de ces courbes :

- Ajouter des couches de BN permet de converger plus vite vers une meilleure solution (prÃ©cision plus Ã©levÃ©e) que lorsque lâ€™on nâ€™en utilise pas ;

Lâ€™amÃ©lioration est dâ€™ailleurs bien plus nette que dans notre exemple du petit jeu de donnÃ© MNIST.

- Ajouter des couches de BN permet dâ€™utiliser au taux dâ€™apprentissage beaucoup plus important (Ã  noter quâ€™avec un taux dâ€™apprentissage 5 fois supÃ©rieur Ã  celui initial, le rÃ©seau Inception diverge dÃ©jÃ ).

On en conclut quâ€™il est plus facile de trouver un taux dâ€™apprentissage â€œacceptableâ€, dans la mesure oÃ¹ lâ€™intervalle de valeur entre le sous-entraÃ®nement et lâ€™explosion de gradient est plus large. 

En outre, un plus grand taux dâ€™apprentissage permet Ã  lâ€™optimiseur dâ€™Ã©viter de sâ€™arrÃªter dans un minimum local. IncitÃ© Ã  lâ€™exploration, lâ€™optimiseur converge vers de meilleures solutions.

- Le modÃ¨le qui ne repose que sur des sigmoÃ¯des atteint des rÃ©sultats compÃ©titifs avec les modÃ¨les qui utilisent des ReLU.

Ce dernier point est davantage intÃ©ressant pour ce quâ€™il reprÃ©sente, que pour les rÃ©sultats obtenus avec la sigmoÃ¯de - qui de toutes Ã©vidences, sont moins bons quâ€™avec la ReLU. 

Pour montrer la valeur de ce rÃ©sultats, je me permets de paraphraser/reformuler les propos de Yann Goodfellow, rÃ©fÃ©rence dans le monde de lâ€™apprentissage profond (inventeur des rÃ©seaux GANs, auteur de lâ€™ouvrage de rÃ©fÃ©rence â€œDeep learning handbookâ€, â€¦ la liste est longue !)  : 
MODIFIER FORMULATION


INSERER CITATION FORME
Avant la BN, les chercheurs pensaient quâ€™il Ã©tait presquâ€™impossible dâ€™entraÃ®ner efficacement des modÃ¨les qui ne reposent que sur des sigmoÃ¯des au niveau des couches cachÃ©es. Plusieurs approches ont Ã©tÃ© envisagÃ©es pour rÃ©soudre les problÃ¨mes dâ€™instabilitÃ© Ã  lâ€™entraÃ®nement, cherchant des mÃ©thodes plus optimales dâ€™initialisation des poids ; les embryons de solutions reposaient sur des dÃ©couvertes heuristiques, fragiles, et peu satisfaisantes. Lâ€™arrivÃ©e de la BN a rendu exploitables des rÃ©seaux que lâ€™on nâ€™arrivaient pas Ã  entraÃ®ner efficacement ; Cet exemple en est une preuve. 
Yann Goodfellow 
(source: la video)

Ces rÃ©sultats donnent un aperÃ§u de lâ€™efficacitÃ© remarquable de la normalisation par lots. Mais cette technique implique quelques effets quâ€™il est important dâ€™avoir Ã  lâ€™esprit pour lâ€™exploiter pleinement.


#### 2) RÃ©gularisation, effet de bord de la normalisation par lots

La normalisation par lots repose sur les valeurs de moyenne et de variance de chaque lot (ou *batch*). Les valeurs dâ€™activations de chaque couche cachÃ©e dÃ©pendent donc du lot actuellement traitÃ© par le rÃ©seau. Cette transformation ajoute donc du bruit liÃ© aux distributions des exemples du lot au niveau de chaque couche cachÃ©e.

Ajouter un peu de bruit dans un rÃ©seau pour Ã©viter le sur-apprentissage â€¦ cela ressemble Ã  un processus de rÃ©gularisation, non ? ;)

En pratique, on ne compte pas sur la normalisation par lot pour Ã©viter le sur-apprentissage dâ€™un rÃ©seau, pour des raisons dâ€™[orthogonalitÃ©s](https://en.wikipedia.org/wiki/Orthogonality_(programming)). Pour faire simple, on sâ€™assure que chacun des modules de notre rÃ©seau remplissent un rÃ´le prÃ©cis, au lieu de compter sur plusieurs modules pour gÃ©rer diffÃ©rents problÃ¨mes en mÃªme temps (ce qui est le meilleur moyen de ne pas aboutir Ã  un solution optimale).

NÃ©anmoins, il est intÃ©ressant dâ€™avoir conscience de ce phÃ©nomÃ¨ne, puisquâ€™il peut expliquer un comportement imprÃ©vu du rÃ©seau (notamment lorsque lâ€™on fait du dÃ©bogage).

<ins>Remarque :</ins> Plus le lot est grand, moins lâ€™effet de rÃ©gularisation sera important (minimisation de lâ€™impact du bruit).


#### 3) ParamÃ¨tres statistiques lors de la phase dâ€™Ã©valuation

Le modÃ¨le est appelÃ© en phase dâ€™Ã©valuation dans deux contextes :
Dans le cadre dâ€™un processus de validation / de test, rÃ©alisÃ©e au cours du dÃ©veloppement et de lâ€™entraÃ®nement du modÃ¨le ;
Lors du dÃ©ploiement de ce dernier en conditions rÃ©elles (phase dâ€™infÃ©rence).

Si dans le premier cas, on peut appliquer la normalisation par lot comme en entraÃ®nement dans un souci de commoditÃ© de calcul, lâ€™appliquer en infÃ©rence nâ€™a vraiment pas de sens. Pourquoi ? Parce que lâ€™on a pas nÃ©cessairement un lot entier Ã  prÃ©dire. Si notre modÃ¨le fonctionne en temps rÃ©el, pour une camÃ©ra embarquÃ©e sur un robot par exemple, on peut nâ€™avoir quâ€™Ã  traiter une image Ã  la fois. Si la taille du lot dâ€™entraÃ®nement est N, que faire des (N - 1) autres valeurs Ã  fournir en entrÃ© pour rÃ©aliser lâ€™infÃ©rence ? 

On peut imaginer que lâ€™on choisit des valeurs arbitraires pour combler le lot. En fournissant le lot nÂ°1 au modÃ¨le, on obtient un certain rÃ©sultat pour lâ€™image qui nous intÃ©resse. Constituons Ã  prÃ©sent un nouveau lot nÂ°2, Ã  partir dâ€™autres valeurs arbitraires ; on obtiendrait un rÃ©sultat diffÃ©rent en sortie. Deux rÃ©sultats diffÃ©rents pour une mÃªme image fournie en entrÃ©e du modÃ¨le nâ€™est certainement pas souhaitable.

NÃ©anmoins, il est nÃ©cessaire dâ€™avoir des valeurs ğœ‡ et Ïƒ pour chacune de nos couches BN, dans la mesure oÃ¹ les paramÃ¨tres ğ›½ et ğ›¾ ont Ã©tÃ© entraÃ®nÃ©es Ã  partir de signaux normalisÃ©es. 

Lâ€™astuce consiste Ã  dÃ©finir ğœ‡pop et Ïƒpop, qui sont respectivement lâ€™estimation de la moyenne et lâ€™Ã©cart-type de la population Ã©tudiÃ©e. Ces paramÃ¨tres sont calculÃ©s comme la moyenne sur lâ€™ensemble des (ğœ‡lot, Ïƒlot) rencontrÃ©s lors des itÃ©rations.

Cependant, cette astuce peut Ãªtre Ã  lâ€™origine dâ€™instabilitÃ© lors de la phase dâ€™Ã©valuation ; voyons cela dans la partie suivante.

#### 4) StabilitÃ© de la couche BN

A REFOMULER : DE FAIT DE ... MAL DIT
Si la normalisation par lots marche gÃ©nÃ©ralement trÃ¨s bien, il arrive parfois que les choses se compliquent. Du fait de la faÃ§on dont cette couche est implÃ©mentÃ©e, il arrive que le rÃ©seau diverge durant la phase dâ€™Ã©valuation.

On a mentionnÃ© plus haut comment sont calculÃ©s ğœ‡pop et Ïƒpop, de faÃ§on Ã  estimer les paramÃ¨tres de normalisation des valeurs dâ€™activation au cours de lâ€™Ã©valuation : on fait la moyenne des (ğœ‡lot, Ïƒlot) vus lors des prÃ©cÃ©dentes itÃ©rations.

Imaginons que lâ€™on entraÃ®ne un rÃ©seau Ã  partir d'images ne contenant que des chaussures de sport. Comment rÃ©agirait le rÃ©seau s'il rencontre des images contenant des chaussures de villes ?

AJOUTER IMAGES
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/car_n_shoes2.jpg">
  Si la distribution d'entrÃ©e durant la phase de test est trop diffÃ©rente de celle de la phase d'entraÃ®nement, le modÃ¨le peut surrÃ©agir Ã  certains signaux, entraÃ®nant les couches d'activations Ã  diverger.
</p>

On devine que les valeurs dâ€™activation au niveau des couches cachÃ©es risquent de suivre des distributions tout Ã  fait diffÃ©rentes - trop, sans doute. Dans ce cas, la paire (ğœ‡pop, Ïƒpop) estimÃ©e au cours de lâ€™entraÃ®nement nâ€™est pas reprÃ©sentative de la population rÃ©elle que rencontre le rÃ©seau en phase de test. Appliquer (ğœ‡pop, Ïƒpop) risque dâ€™Ã©loigner le signal de la loi normale centrÃ©e rÃ©duite dÃ©sirÃ©e, pouvant mener Ã  une surestimation des valeurs dâ€™activation. 

Ce phÃ©nomÃ¨ne est amplifiÃ© par une propriÃ©tÃ© connue de la couche BN : au cours de lâ€™entraÃ®nement, les valeurs d'activation sont normalisÃ©es en tenant compte de leur propre valeur. Au moment de lâ€™infÃ©rence, on applique la normalisation Ã  partir des coefficients (ğœ‡pop, Ïƒpop) calculÃ© pendant lâ€™entraÃ®nement : les coefficients utilisÃ©s pour la normalisation ne tiennent alors pas compte des valeurs dâ€™activations elles-mÃªme.

En gÃ©nÃ©ral, on sâ€™assure que les jeux de donnÃ©es dâ€™entraÃ®nement et de tests soient suffisamment proches pour que (ğœ‡pop, Ïƒpop) soient cohÃ©rents. Dans le cas inverse, on pourrait penser que le jeu dâ€™entraÃ®nement nâ€™est pas suffisamment large et de bonne qualitÃ© pour entraÃ®ner notre modÃ¨le sur la tÃ¢che dÃ©sirÃ©e.

Mais il existe des cas oÃ¹ ce problÃ¨me survient (lien : https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/38 ), jâ€™en ai moi mÃªme fait les frais : Au cours de la compÃ©tition Kaggle de prÃ©diction de lâ€™Ã©volution de la maladie de fibrose pulmonaire (lien : https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression), nous disposions dâ€™un petit jeu de donnÃ©e dâ€™entraÃ®nement contenant - entres autres - des scanners 3D des poumons de chaque patient. Le contenu Ã©tait si riche et si divers (pour une petite centaine dâ€™exemples), que le rÃ©seau convolutif avec lequel je comptais faire de lâ€™extraction de caractÃ©ristiques mâ€™a fait la fÃ¢cheuse surprise de retourner des valeurs astronomiques sitÃ´t que lâ€™entraÃ®nement se trouvait en phase de validation...un rÃ©gale Ã  dÃ©boguer. ;)

Dans ce genre de contexte oÃ¹ les jeux de donnÃ©es dâ€™entraÃ®nement sont limitÃ©s, il faut faire avec les moyens du bord. 

Ajouter systÃ©matiquement des BN dans notre rÃ©seau - en pensant que cela nâ€™aura que des effets positifs - nâ€™est certainement pas la meilleure stratÃ©gie !

#### 5) RÃ©seaux rÃ©currents, et normalisation par couches

En pratique, il est largement admis le principe suivant :
Pour les rÃ©seaux convolutifs (CNN) : utiliser de prÃ©fÃ©rence la Normalisation par Lots (Batch Normalization, notÃ©e BN)
Pour les rÃ©seaux rÃ©currents (RNN) : utiliser de prÃ©fÃ©rence la Normalisation par Couches (Layer Normalization, notÃ©e LN)

Si la BN normalise Ã  lâ€™Ã©chelle des exemples de chaque lot, la LN normalise Ã  lâ€™Ã©chelle des couches cachÃ©es. Cette deuxiÃ¨me solution sâ€™avÃ¨re Ãªtre plus efficace avec des rÃ©seaux rÃ©currents. Une piste dâ€™intuition rÃ©side dans la difficultÃ© Ã  dÃ©finir une stratÃ©gie cohÃ©rente avec ce type de neurones, qui repose sur la multiplication dâ€™une mÃªme matrice de poids de nombreuses fois successivement. Faut-il normaliser indÃ©pendamment chaque Ã©tape ? Ou au contraire, en faire la moyenne, puis appliquer la normalisation rÃ©cursivement ?


Je ne mâ€™attarderai pas davantage sur ce point, ce nâ€™est pas prÃ©cisÃ©ment lâ€™objet de cet article.


#### 6) Avant ou aprÃ¨s la fonction non-linÃ©aire ?

Historiquement, la couche BN est positionnÃ©e juste avant la fonction non-linÃ©aire. Ceci Ã©tant cohÃ©rent avec les objectifs et les hypothÃ¨ses des auteurs Ã  lâ€™Ã©poque. 

Dans leur article, ils dÃ©clarent :

AJOTUER CITATION
â€œNotre voudrions Ãªtre certains que le rÃ©seau produise toujours une activation avec une distribution statistique dÃ©sirÃ©e.â€
Sergey Ioffe & Christian Szegedy
source : [1]


En revanche, des expÃ©rimentations ont montrÃ© que la couche BN positionnÃ©e aprÃ¨s la fonction non-linÃ©aire donne de meilleurs rÃ©sultats.

Cette petite expÃ©rience en est [un exemple](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu).


FranÃ§ois Chollet, crÃ©ateur de Keras et actuellement ingÃ©nieur chez Google, a dâ€™ailleurs prÃ©tendu Ã  ce sujet que :
AJOUTER CITATION

â€œJe nâ€™ai pas vÃ©rifiÃ© ce qui est suggÃ©rÃ© dans lâ€™article original, mais je peux garantir avoir vu dans du code Ã©crit rÃ©cemment par Christian [Szegedy] que la ReLU est appliquÃ©e avant la BN. Mais câ€™est parfois encore sujet Ã  dÃ©bat.â€
FranÃ§ois Chollet
source :  https://github.com/keras-team/keras/issues/1802



MÃªme si le vent semble tourner, beaucoup dâ€™architectures communÃ©ment utilisÃ©es pour de lâ€™apprentissage par transfert (ResNet, mobilenet-v2, ...) placent toujours BN avant.

Remarquez que lâ€™article [2] - qui remet en question les intuitions dÃ©fendues par lâ€™article original [1] pour expliquer lâ€™efficacitÃ© de la couche BN (voir C.III.3) - ont placÃ© la couche BN avant la fonction dâ€™activation. Ils nâ€™apportent toutefois aucun Ã©lÃ©ment dâ€™explication sur cet aspect.

Ã€ ma connaissance, cette question est donc toujours en discussion. 


<ins>Pour en savoir plus :</ins> [Conversation  intÃ©ressante](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/dgqaksn/) (hÃ©las en anglais) sur reddit - mÃªme si certains arguments sont fragiles - avec une grosse tendance en faveur de la BN aprÃ¨s lâ€™activation.


### III) Pourquoi la couche BN est-elle efficace ?

#### 1) PremiÃ¨re hypothÃ¨se - Confusion autour du dÃ©calage de covariable interne

Bien que fondamental, la normalisation par lots est un concept souvent mal compris. Cela tient plus dâ€™une erreur longtemps propagÃ©e, que de la complexitÃ© de la notion.

Dans lâ€™article officiel, les auteurs introduisent la BN comme suit : 

AJOUTER CITATION
â€œNous appelons DÃ©calage de Covariable Interne (en anglais Internal Covariate Shift) la modification au cours de lâ€™entraÃ®nement de la distribution statistique des noeuds internes dâ€™un rÃ©seau profond. [...] Nous proposons un nouveau mÃ©canisme, que lâ€™on appelle Normalisation par Lots (Batch Normalization), qui rÃ©sout en partie le problÃ¨me du dÃ©calage de covariable interne, et se faisant accÃ©lÃ¨re significativement lâ€™entraÃ®nement des rÃ©seaux de neurones profonds.â€
Sergey Ioffe & Christian Szegedy
source : [1] article officiel


Autrement dit, lâ€™efficacitÃ© de la couche BN rÃ©side dans sa rÃ©solution (partielle) du problÃ¨me de dÃ©calage de covariable interne.

AJOUTER LIEN
Ce point Ã  Ã©tÃ© remis en question dans des recherches postÃ©rieures (liens).

Pour comprendre ce qui a suscitÃ© cette confusion, intÃ©ressons-nous Ã  ce quâ€™est le dÃ©calage de covariable, et aux effet de la normalisation par lot sur un rÃ©seau de neurones profond.

Notation : Lâ€™abrÃ©viation ICS fait rÃ©fÃ©rence au DÃ©calage de Covariable Interne (venant de lâ€™anglais Internal Covariate Shift). 

BONNE TAILLE DE TITRE ?
##### Quâ€™est-ce que le dÃ©calage de covariable (au sens de la distribution) ?

Les auteurs lâ€™ont dit : le dÃ©calage de covariable, au sens de la distribution, dÃ©crit la modification de distribution statistique au cours de lâ€™entraÃ®nement dâ€™un modÃ¨le, et, par extension, le dÃ©calage de covariable interne dÃ©crit ce phÃ©nomÃ¨ne Ã  lâ€™intÃ©rieur dâ€™un rÃ©seau de neurone profond.

Voyons en quoi cela pourrait poser problÃ¨me avec un exemple.

Supposons que lâ€™on cherche Ã  entraÃ®ner un rÃ©seau classificateur qui puisse rÃ©pondre Ã  la question suivante : Cette image contient-elle une voiture ? Si lâ€™on voulait extraire toutes les images de voiture dâ€™une immense base de donnÃ©e non-Ã©tiquetÃ©e, un tel rÃ©seau serait trÃ¨s efficace. 

On aurait bien-sÃ»r une image RGB en entrÃ©e, un ensemble de couches de neurones convolutifs, suivis de quelques couches entiÃ¨rement connectÃ©es (perceptrons). On souhaite obtenir en sortie une seule valeur flottante comprise entre 0 et 1, dÃ©crivant la probabilitÃ© que lâ€™image contienne effectivement une voiture.

AJOUTER SCHÃ‰MA 5
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_5fr.jpg">
  <strong>SchÃ©ma 5 : RÃ©seau convolutif simple pour rÃ©aliser une tÃ¢che de classification. </strong>
</p>


Pour entraÃ®ner un tel modÃ¨le, il nous faudrait un nombre consÃ©quent dâ€™images Ã©tiquetÃ©es (1 : â€œCette image contient une voiture.â€, ou 0 : â€œCette image ne contient pas de voiture).

Mais imaginons que nous ne disposions que de voiture â€œclassiquesâ€ (de ville, ou de sport) pour l'entraÃ®nement. Comment le modÃ¨le rÃ©agirait si nous lui demandions de classifier une image contenant une formule 1 ?

AJOUTER IMAGES
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/car_n_shoes.jpg">
  Comme Ã©voquÃ© dans la section (section C.2.4), le dÃ©calage de distribution peut dÃ©tÃ©riorer les performances du rÃ©seau, voir provoquer une explosion des valeurs d'activation.
</p>


Dans cet exemple, il y a un dÃ©calage entre la distribution statistique associÃ©es aux images de voitures utilisÃ©es pour lâ€™entraÃ®nement, et la distribution statistique associÃ©es aux images de voitures de test. Plus gÃ©nÃ©ralement, il suffit dâ€™une autre orientation, forme, luminositÃ© ou condition climatique que celles vues pendant la phase dâ€™entraÃ®nement pour que nos performances se gÃ¢tent. On dit alors que notre modÃ¨le ne gÃ©nÃ©ralise pas efficacement.


Si on reprÃ©sentait les caractÃ©ristiques extraites par notre modÃ¨le dans lâ€™espace de caractÃ©ristique, on aurait sans doute quelque chose comme Ã§a :

AJOUTER SCHÃ‰MA 6.a
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_6afr.jpg">
  <strong>SchÃ©ma 6.a : Pourquoi faut-il normaliser les valeur dâ€™entrÃ©e dâ€™un modÃ¨le ? cas non-normalisÃ©.</strong> Ã€ lâ€™entraÃ®nement, les valeurs dâ€™entrÃ©e sont trÃ¨s Ã©parses : la fonction approximÃ©e sera prÃ©cise lÃ  oÃ¹ la densitÃ© de points est forte. Au contraire, elle sera imprÃ©cise lÃ  oÃ¹ la densitÃ© est faible (pouvant prendre lâ€™une des courbes tracÃ©es Ã  titre dâ€™exemple).
</p>


ConsidÃ©rons que le symbole croix corresponde aux caractÃ©ristiques associÃ©es Ã  une image ne contenant pas une voiture, et que le symbole rond corresponde aux caractÃ©ristiques associÃ©es Ã  une image contenant une voiture. On peut voir quâ€™une mÃªme fonction sÃ©parerait efficacement les deux ensembles. Mais il y a fort Ã  parier que notre modÃ¨le dÃ©duise du jeu dâ€™entraÃ®nement une fonction moins prÃ©cise pour la partie supÃ©rieure du graphique, puisquâ€™il nâ€™y a pas de valeur dâ€™entraÃ®nement qui se situe dans cette zone pour servir de repÃ¨re Ã  lâ€™optimiseur. Ce dernier approximera la fonction du mieux quâ€™il pourra, poussant le classificateur Ã  faire beaucoup dâ€™erreurs. 

EntraÃ®ner efficacement notre rÃ©seau nÃ©cessiterait beaucoup dâ€™images de voitures, de sorte que notre jeu dâ€™entraÃ®nement contiennent Ã  peu prÃªt toutes les variations de positions et de contexte imaginable. MÃªme si dans les faits, câ€™est de cette faÃ§on que lâ€™on entraÃ®ne de bons rÃ©seaux de neurones aujourdâ€™hui, on aimerait bien que nos modÃ¨les puisse gÃ©nÃ©raliser Ã  partir du plus petit nombre dâ€™exemple possible.

Le problÃ¨me pourrait Ãªtre rÃ©sumÃ© ainsi :

AJOUTER CITATION

Du point de vu du modÃ¨le, les images sont trop diffÃ©rentes les unes des autres. Autrement dit, leurs paramÃ¨tres statistiques sont trop diffÃ©rents. 

On dit quâ€™il y a dÃ©calage de covariable [au sens de la distribution] (en anglais covariate shift). 


On retrouve ce mÃªme problÃ¨me dans des cas plus simples que celui des rÃ©seaux de neurones profonds, comme lors de rÃ©gressions linÃ©aires. Il est apparu beaucoup plus facile de rÃ©soudre des problÃ¨mes de rÃ©gression lorsque le jeu dâ€™entraÃ®nement suit une loi normale centrÃ©e rÃ©duite (moyenne = 0, Ã©cart-type = 1) ; câ€™est pourquoi il est trÃ¨s frÃ©quent de normaliser les valeurs dâ€™entrÃ©es dâ€™un modÃ¨le.

AJOUTER SCHÃ‰MA 6.b
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_6bfr.jpg">
  <strong>SchÃ©ma 6.b : Pourquoi faut-il normaliser les valeur dâ€™entrÃ©e dâ€™un modÃ¨le ? cas normalisÃ©.</strong> Le signal dâ€™entrÃ© normalisÃ© rend les valeurs moins Ã©parses Ã  lâ€™entraÃ®nement : il sera plus facile de trouver une fonction gÃ©nÃ©ralisante. 
</p>

Cette solution Ã©tait dÃ©jÃ  connue et mise en pratique avant la publication de lâ€™article qui nous intÃ©resse ici. La couche de BN, elle, considÃ¨re ce problÃ¨me au niveau des couches cachÃ©es.

BONNE TAILLE DE TITRE ?
##### Le dÃ©calage de covariable interne, hypothÃ¨se dÃ©fendue par lâ€™article original


AJOUTER SCHÃ‰MA 7
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_7fr.jpg">
  <strong>SchÃ©ma 7 : Principe du dÃ©calage de covariable (ICS)</strong> au sens de la distribution (ICSdistrib).
</p>

Dans notre exemple du classificateur de voiture, on peut envisager les couches cachÃ©es comme des unitÃ©s qui sâ€™activent lorsquâ€™elle identifient certaines caractÃ©ristiques â€œconceptuellesâ€ associÃ©es Ã  la voiture : par exemple une roue, un pneu, ou une portiÃ¨re. On peut supposer que le mÃªme phÃ©nomÃ¨ne prÃ©cÃ©demment dÃ©crit a lieu au niveau des couches cachÃ©es. Un pneu orientÃ© dâ€™une certaine faÃ§on activera un neurone selon une certaine distribution. On souhaite alors quâ€™un autre pneu, mÃªme orientÃ© diffÃ©remment, puisse activer le mÃªme neurone avec une distribution statistique comparable, afin que le rÃ©seau puisse en tirer des conclusions sur la probabilitÃ© que lâ€™image de dÃ©part contienne une voiture.

Si le signal dâ€™entrÃ© prÃ©sente un grand dÃ©calage de covariable (câ€™est Ã  dire si sa distribution statistique varie beaucoup dâ€™un passage Ã  lâ€™autre), lâ€™optimiseur aura plus de difficultÃ© Ã  gÃ©nÃ©raliser - autrement dit Ã  apprendre - Ã  partir de caractÃ©ristiques communes. Ã€ lâ€™inverse, en suivant une distribution proche de la loi centrÃ©e rÃ©duite, lâ€™optimiseur pourra plus facilement approximer une fonction gÃ©nÃ©ralisante. Les auteurs appliquent donc la mÃªme stratÃ©gie Ã  lâ€™Ã©chelle des couches cachÃ©es pour aider le rÃ©seau Ã  gÃ©nÃ©raliser Ã  des niveaux de caractÃ©ristiques plus â€œconceptuelsâ€.


NÃ©anmoins, il nâ€™est pas souhaitable que tous nos signaux dâ€™activations suivent une loi normal centrÃ©e rÃ©duite. Cela limiterait sa capacitÃ© de reprÃ©sentativitÃ©, et pour cause :


AJOUTER SCHEMA 8
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_8.jpg">
  <strong>SchÃ©ma 8 : Pourquoi il nâ€™est pas souhaitable de contraindre lâ€™activation Ã  une loi normale centrÃ©e rÃ©duite.</strong> La sigmoÃ¯de ne fonctionne ici quâ€™en rÃ©gime linÃ©aire.
</p>


Si lâ€™on prend lâ€™exemple donnÃ© de lâ€™article original, la sigmoÃ¯de, un signal dâ€™entrÃ© compris en 0 et 1 limiterait la fonction non-linÃ©aire Ã  son rÃ©gime â€¦ linÃ©aire. 

Pour pallier Ã  ce problÃ¨me, les auteurs ont alors ajoutÃ©s deux paramÃ¨tres, ğ›½ et ğ›¾, pour permettre Ã  lâ€™optimiseur de dÃ©finir lui mÃªme la moyenne (via ğ›½) et lâ€™Ã©cart type (via ğ›¾) optimal pour une tÃ¢che donnÃ©.

**âš Nous arrivons au point qui est souvent lâ€™objet de confusion.** Pendant quelques annÃ©es aprÃ¨s la sortie de lâ€™article original, on a dÃ©duit de lâ€™efficacitÃ© de la couche BN lâ€™explication suivante :

TROUVER UNE MISE EN PAGE 
HypothÃ¨se 1 : 

BN -> normalisation du signal Ã  chaque couche cachÃ©e -> ajout de deux paramÃ¨tres Ã  ajuster pour profiter de tous les rÃ©gimes dâ€™activations -> facilite lâ€™entraÃ®nement

Ce qui situe dâ€™intÃ©rÃªt de la BN dans le fait que cette couche assure une distribution proche dâ€™un loi normale centrÃ©e rÃ©duite, facilitant la gÃ©nÃ©ralisation. Ceci a Ã©tÃ© remis en question, prÃ©fÃ©rant une autre explication que lâ€™on pourrait Ã©noncer comme suit :


HypothÃ¨se 2 :

BN -> normalisation du signal Ã  chaque couche cachÃ©e -> diminue lâ€™interdÃ©pendance des couches cachÃ©es entre elles sur les paramÃ¨tres statistiques -> facilite lâ€™entraÃ®nement

Ce nâ€™est plus tout Ã  fait la mÃªme chose. Ici, le passage Ã  la loi normale centrÃ©e rÃ©duite nâ€™est plus quâ€™un moyen de rÃ©duire l'interdÃ©pendance des couches les unes avec les autres. Ã‰tudions cette nouvelle hypothÃ¨se.



#### 2) DeuxiÃ¨me hypothÃ¨se : limiter lâ€™interdÃ©pendance de distributions 

*Note de rÃ©daction : Ne disposant pas de preuves irrÃ©futables, je me permets de mâ€™appuyer trÃ¨s largement sur les explications de Yann Goodfellow Ã  ce sujet (quâ€™il exprime dans cet brillante vidÃ©o (https://www.youtube.com/watch?v=Xogn6veSyxA)), et sur quelques discussions en ligne citÃ©es en rÃ©fÃ©rences.*

ConsidÃ©rons lâ€™exemple suivant :

AJOUTER SCHÃ‰MA 9
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_9fr.jpg">
  <strong>SchÃ©ma 9 : Principe simplifiÃ© dâ€™un rÃ©seau de neurone profond,</strong> composÃ© uniquement de transformation linÃ©aires.
</p>



OÃ¹ (a), (b), (c), (d) et (e) sont les couches successives dâ€™un rÃ©seau de neurones. Notre cas est trÃ¨s simple, il sâ€™agit dâ€™un rÃ©seau constituÃ© dâ€™une succession de transformations linÃ©aires. On cherche Ã  entraÃ®ner ce rÃ©seau avec la mÃ©thode de descente de gradient (Stochastic Gradient Descent, SGD).

Pour calculer la mise Ã  jour des poids de (a), on calcule le gradient en partant de la fin. On obtient :
grad(a) = b * c * d * e

On se place dâ€™abord dans le cas dâ€™un rÃ©seau sans couche BN. On conclut de lâ€™Ã©quation Ã©tablie ci-dessus que si tous les gradients sont grands, grad(a) aura une valeur trÃ¨s Ã©levÃ©e. Ã€ lâ€™inverse, des gradients trÃ¨s petits sur les couches suivantes forceront grad(a) vers une valeur presque nÃ©gligeable. 

Si lâ€™on sâ€™intÃ©resse au distributions statistiques qui se prÃ©sentent Ã  lâ€™entrÃ©e de chacune de ces couches, on sâ€™aperÃ§oit de lâ€™interdÃ©pendance entre les couches du rÃ©seau : Une modification des poids de (a) modifiera la distribution du signal entrant dans (b), qui aura Ã  terme des consÃ©quences sur celles des signaux entrant dans (d) et (e). Ceci est problÃ©matique pour la stabilisation de lâ€™entraÃ®nement : pour ajuster la distribution statistique dâ€™une couche, il faut tenir compte de lâ€™ensemble de la chaÃ®ne. 

Or, la SGD est une mÃ©thode qui sâ€™intÃ©resse aux relations du 1er ordre (dÃ©rivÃ©e premiÃ¨re appliquÃ©e Ã  une couche par rapport Ã  la prÃ©cÃ©dente). Elle ne tient donc pas compte des interactions mentionnÃ©es prÃ©cÃ©demment !


AJOUTER ENCART

Approfondissement : Il existe des algorithmes qui utilisent des relations du 2e ordre pour tenir compte de ces effets. On peut par exemple prendre les dÃ©rivÃ©es 2nd pour voir comment interagit (a) avec lâ€™ensemble des paramÃ¨tres. On peut alors exploiter lâ€™inverse de la matrice hessienne pour normaliser lâ€™impact de chaque paramÃ¨tre sur les autres. On obtient des rÃ©sultats acceptables si lâ€™on nâ€™a que des interactions par paires, autrement dit si (a) est directement affectÃ© par (b), (c), (d) ou (e). 

Mais si lâ€™on calcule les dÃ©rivÃ©es de (b) par rapport aux autres paramÃ¨tres, on a : 

grad(b) = c * d * e

Or, les paramÃ¨tres de ces trois couches peuvent eux-mÃªme interagir entre eux !

Si on considÃ¨re des mÃ©thodes du 2e ordre, comme la mÃ©thode de Newton, qui nÃ©cessite de calculer la matrice hessienne, lâ€™algorithme dâ€™apprentissage devient beaucoup plus lourd en calculs : la complexitÃ© devient quadratique avec le nombre de paramÃ¨tres pour calculer la hessienne, on est bien loin de la complexitÃ© linÃ©aire avec le nombre de paramÃ¨tre lors de la descente de gradient. 

Tout cela pour finalement ne tenir compte que des interactions du 2e ordre. Dans notre exemple pourtant simple, nous avons des interactions du 5e ordre ! Il nâ€™y aurait pas mÃªme de moyens directs de rÃ©soudre des interactions du 5e ordre avec un rÃ©solveur algÃ©brique linÃ©aire (linear algebric solver) pour gÃ©rer toute ces interactions. 

Imaginez pour des rÃ©seaux trÃ¨s profonds ...


AJOUTER SCHÃ‰MA 10
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_10fr.jpg">
  <strong>SchÃ©ma 10 : Principe de lâ€™hypothÃ¨se nÂ°2.</strong> En normalisant puis ajustant le signal avec ğ›½ et ğ›¾, la couche BN simplifie le contrÃ´le du signal au niveau de chaque couche cachÃ©e.
</p>



Ajouter la couche BN attÃ©nue trÃ¨s largement lâ€™interdÃ©pendance entre les couches pendant lâ€™apprentissage. La normalisation agit comme une porte que lâ€™optimiseur peut ajuster Ã  partir des seuls paramÃ¨tres ğ›½ et ğ›¾. Il nâ€™est alors plus nÃ©cessaire de tenir compte de tous les paramÃ¨tres du rÃ©seau pour avoir des informations statistique sur une couche cachÃ©e.

<ins>Remarque :</ins> Lâ€™optimiseur peut alors se permettre de faire de bien plus grosses modifications de poids sur chacune des couches, sans que cela nâ€™altÃ¨re le travail rÃ©alisÃ© sur les couches successives. Il est donc beaucoup plus facile de dÃ©terminer des hyperparamÃ¨tres qui convergeront vers une solution optimale.

AJOUTER ENCARTS
Cet exemple met de cÃ´tÃ© lâ€™hypothÃ¨se dans laquelle la BN servirait Ã  faire tendre les valeurs dâ€™activations des couches cachÃ©es vers une loi normale centrÃ©e rÃ©duite. 

Ici, il sâ€™agit de faciliter le travail de lâ€™optimiseur en lui permettant dâ€™ajuster les distributions statistiques internes en jouant sur seulement deux paramÃ¨tres Ã  la fois.

Il sâ€™agit nÃ©anmoins dâ€™intuitions autour du fonctionnement de la normalisation par lot, et il nâ€™existe pas, Ã  ma connaissance, de solides preuves de ces hypothÃ¨ses. 

Un article paru en 2019 par une Ã©quipe du MIT a apportÃ©e une contribution intÃ©ressante Ã  la comprÃ©hension de lâ€™efficacitÃ© de la couche BN. Les auteurs remettent trÃ¨s fortement en question le lien entre lâ€™efficacitÃ© de la couche BN et la rÃ©duction du dÃ©calage de covariable interne, au sens de la distribution (premiÃ¨re hypothÃ¨se) !


#### 3) TroisiÃ¨me hypothÃ¨se - lissage du paysage dâ€™optimisation :

Note de rÃ©daction : Dans cette partie, je mâ€™efforce de synthÃ©tiser lâ€™article [2], pour prÃ©senter leurs principales conclusions quant aux propriÃ©tÃ©s de la couche BN. Cet article est dense, je vous invite Ã  vous y pencher avec plus dâ€™attention si ces concepts vous intÃ©ressent.  

IntÃ©ressons-nous directement Ã  la deuxiÃ¨me expÃ©rience de cet article. Les auteurs entraÃ®nent trois rÃ©seaux VGG (sur CIFAR-10) :
Le premier sans couche BN ;
Le deuxiÃ¨me avec des couches BN ;
Le troisiÃ¨me est identique au deuxiÃ¨me, Ã  ceci prÃªt quâ€™ils ajoutent explicitement de lâ€™ICS au niveau des couches cachÃ©es en ajoutant du bruit (valeurs alÃ©atoires ajoutÃ©es/multipliÃ©es Ã  la moyenne/variance) ; 

Ils observent ensuite la prÃ©cision obtenue par chaque modÃ¨le, ainsi que lâ€™Ã©volution des distributions dâ€™activations au niveau des couches cachÃ©es. Voici les rÃ©sultats obtenus :


AJOUTER GRAPHIQUE 6
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_6.png">
  <strong>Graphique 6 : Impact de la couche BN sur lâ€™ICSdistrib</strong> (source : [2]). Les deux rÃ©seaux qui utilisent la couche BN sâ€™entraÃ®nent plus vite que le rÃ©seau standard ; ajouter explicitement de lâ€™ICSdistrib sur un rÃ©seau normalisÃ© ne dÃ©tÃ©riore pas ces propriÃ©tÃ©s.
</p>


On observe que le 3e rÃ©seau a, comme prÃ©vu, un trÃ¨s fort ICS. Pourtant, cela ne lâ€™empÃªche pas dâ€™Ãªtre entraÃ®nÃ© de maniÃ¨re plus rapide et plus stable que le rÃ©seau standard. Les performances sont assez similaires au rÃ©seau avec des couches BN mais sans ajout explicit dâ€™ICS, suggÃ©rant que lâ€™efficacitÃ© de la BN nâ€™est pas liÃ© Ã  la diminution de lâ€™ICS, comme le soutient lâ€™hypothÃ¨se 1.

Nâ€™Ã©cartons pas lâ€™ICS trop vite : la dÃ©finition du dÃ©calage de covariable interne (donnÃ©e dans lâ€™article original de la couche BN) liÃ©e Ã  la distribution est peut-Ãªtre insatisfaisante. Les auteurs de [2] ont explorÃ©s une autre dÃ©finition de lâ€™ICS, cette fois-ci exprimant les propriÃ©tÃ© dâ€™optimisation du modÃ¨le. En voici une dÃ©finition :


AJOUTER ENCART
ConsidÃ©rons une entrÃ©e fixe Ã  notre modÃ¨le, notÃ©e X. 

On dÃ©finit le dÃ©calge de covariable interne dâ€™un point de vu de lâ€™optimisation (notÃ© ICSopti ), la diffÃ©rence entre le gradient calculÃ© au niveau dâ€™une couche k aprÃ¨s avoir rÃ©tropropagÃ© lâ€™erreur L(X)It, et le gradient calculÃ© au niveau de la mÃªme couche k aprÃ¨s la mise Ã  jour des poids des couches prÃ©cÃ©dentes L(X)It+1.


Cette dÃ©finition a pour but de focaliser lâ€™attention sur le gradient de lâ€™erreur, plus que sur la distribution des valeurs dâ€™activation. On cherche ainsi Ã  sâ€™intÃ©resser directement au problÃ¨me dâ€™optimisation sous-jacent pour comprendre lâ€™efficacitÃ© de la couche BN, et voir le lien que peut avoir lâ€™ICS sur lâ€™entraÃ®nement.

Lâ€™expÃ©rience suivante Ã©value cette nouvelle approche de lâ€™ICS. Pour cela, les auteurs Ã©valuent lâ€™impact de la normalisation par lots sur lâ€™ICSopti en regardant son Ã©volution au cours de lâ€™entraÃ®nement dâ€™un rÃ©seau avec / sans couche BN. Pour quantifier la diffÃ©rence entre les gradients Ã©voquÃ©es dans la dÃ©finition de lâ€™ICSopti , les auteurs calculent :
La diffÃ©rence L2 : Les gradients ont-ils une norme proche avant et aprÃ¨s la mise Ã  jour des poids ? IdÃ©alement : L2-diff = 0 ;
Le cosinus de lâ€™angle orientÃ© : Les gradients ont-ils une direction similaire avant et aprÃ¨s la mise Ã  jour des poids ? IdÃ©alement: cos(grad(k)It , grad(k)It+1) = 1 .

AJOUTER GRAPHIQUE 7
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_7.png">
  <strong>Graphique 7 : Impact de la couche BN sur lâ€™ICSopti</strong> (source : [2]). Les diffÃ©rence de normes et dâ€™angles de gradient suggÃ¨re quâ€™elle nâ€™empÃªche pas le dÃ©calage ; le phÃ©nomÃ¨ne semble au contraire sâ€™aggraver.
</p>

Les rÃ©sultats sont surprenants : Le rÃ©seau qui repose sur des couches de normalisation par lots a un dÃ©calage de covariable interne similaire, voir supÃ©rieur, au rÃ©seau standard. Rappelons-le, le rÃ©seau qui utilise des couche de BN (courbe bleue) sâ€™entraÃ®ne beaucoup plus vite et converge vers une meilleure solution (courbe rouge) !

DÃ©cidÃ©ment, lâ€™ICS - dans les dÃ©finitions quâ€™on en a donnÃ© - nâ€™a pas lâ€™air liÃ© aux performances dâ€™entraÃ®nement.

La normalisation par lots aurait donc dâ€™autres effets sur lâ€™entraÃ®nement, qui aboutissent Ã  une convergence plus rapide vers une meilleure solution.

IntÃ©ressons nous directement au problÃ¨me de lâ€™optimisation : quel est lâ€™impact de la couche BN sur le paysage dâ€™optimisation (en anglais : optimization landscape) ?

Voici la derniÃ¨re expÃ©rience que nous allons aborder dans cet article :

AJOUTER SCHÃ‰MA 11
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/sbn_11.jpeg">
  <strong>SchÃ©ma 11 : Exploration du paysage dâ€™optimisation</strong> dans la direction du gradient. ExpÃ©rience menÃ©e dans lâ€™article [2].
</p>

Ã€ partir dâ€™un mÃªme gradient, on rÃ©alise la mise Ã  jour des poids pour diffÃ©rents pas dâ€™optimisation (comparable Ã  une augmentation du taux dâ€™apprentissage !). Intuitivement, on dÃ©finit une direction Ã  partir dâ€™un certain point de lâ€™hyperplan dans lâ€™espace des paramÃ¨tres, puis on explore le paysage dâ€™optimisation en suivant cette direction de plus en plus loin. 

Ã€ chaque pas, on relÃ¨ve le gradient et la perte. On peut donc comparer les diffÃ©rents point du paysage dâ€™optimisation avec le point de dÃ©part. Si lâ€™on relÃ¨ve de fortes variations, le paysage est trÃ¨s instable et le gradient est incertain : de grands pas risquerait de dÃ©tÃ©riorer notre optimisation. Au contraire, si les variations relevÃ©es sont petites, le paysage est stable et le gradient est plus sÃ»r : on peut alors se permettre de plus grands pas sans compromettre lâ€™optimisation ! Autrement dit, on peut appliquer un plus grand taux dâ€™apprentissage, et atteindre une convergence plus rapide. Ceci Ã©tant des propriÃ©tÃ©s bien connus des utilisateurs de la couche BNâ€¦

Place aux rÃ©sultats :


AJOUTER GRAPHIQUE 8
<p align="center">
  <img src="https://raw.githubusercontent.com/Johann-Huber/Johann-Huber.github.io/master/assets/gbn_8.png">
  <strong>Graphique 8 : Impact de la couche BN sur le lissage du paysage dâ€™optimisation</strong> (source : [2]). Avec la normalisation par lots, on constate lâ€™attÃ©nuation des fortes variations du gradient.
</p>

On peut voir trÃ¨s distinctement que le paysage dâ€™optimisation est bien plus lisse avec la couche BN que sans. 

Nous tenons enfin une piste dâ€™explication : Par un moyen ou un autre, la couche de normalization lisse de paysage dâ€™optimisation. Le travail de lâ€™optimiseur en est grandement facilitÃ© : on peut dÃ©finir un taux dâ€™apprentissage plus important, Ã©tant moins soumis au risque de disparition de gradient (poids bloquÃ©s sur une hypersurface plate) ou Ã  lâ€™explosion de gradient (poids entraÃ®nÃ© par un minimum local abrupt).

Nous sommes Ã  prÃ©sent en mesure de formuler la troisiÃ¨me hypothÃ¨se, dÃ©fendue par cet article [2] :


HypothÃ¨se 3 :

BN -> normalisation du signal Ã  chaque couche cachÃ©e -> lisse le paysage dâ€™optimisation -> entraÃ®nement plus stable et plus rapide.


Une nouvelle interrogation sâ€™impose : par quel moyen la normalisation par lots lisse-t-elle le paysage dâ€™optimisation ?

Pour finir, les auteurs ont constatÃ© que cet effet nâ€™est pas unique Ã  la normalisation par lots, obtenant des performances dâ€™entraÃ®nement comparable avec dâ€™autres formes de normalisation (par exemple la normalisation L1 ou L2). Les bonnes performances de la normalisation par lot seraient donc fortuites, mettant en oeuvre un mÃ©canisme dont nous nâ€™avons pas encore saisi tous les ressorts. Par ailleurs, leur article explore dâ€™un point de vu thÃ©orique les consÃ©quences de la normalisation par lots sur les propriÃ©tÃ©s de continuitÃ©s de la fonction de coÃ»t. Ils montrent que la normalisation rend la fonction Lipschitzienne.

En dÃ©finitive, cet article bat en brÃ¨che lâ€™idÃ©e communÃ©ment admise que lâ€™efficacitÃ© de la couche BN reposerait sur lâ€™attÃ©nuation du dÃ©calage de covariable interne (au sens de la distribution comme au sens de lâ€™optimisation). En revanche, il souligne lâ€™effet de lissage du paysage dâ€™optimisation que la normalisation implique. 

Si cet article Ã©nonce une hypothÃ¨se quant Ã  la raison pour laquelle lâ€™entraÃ®nement est plus rapide, il nâ€™apporte pas dâ€™Ã©lÃ©ment Ã  propos des propriÃ©tÃ©s gÃ©nÃ©ralisantes de la couche BN. 

Une hypothÃ¨se Ã  ce sujet est briÃ¨vement Ã©voquÃ© en fin dâ€™article, soutenant que le lissage du paysage dâ€™optimisation permettrait au modÃ¨le de converger vers des minimums plats, ayant de meilleures propriÃ©tÃ©s gÃ©nÃ©ralisantes.

Soulignons cependant que leur principal contribution est la remise en question de la vision communÃ©ment admise depuis la sortie de lâ€™article officiel - ce qui est, dÃ©jÃ , significatif.


#### 4) Bilan : Pourquoi la BN est efficace ? Ce que lâ€™on sait aujourdâ€™hui


A VERIFIER
| HypothÃ¨se nÂ°1 :                                                                                                            | HypothÃ¨se nÂ°2 :                                                                                                                                                                                            |                                                                                                      HypothÃ¨se nÂ°3                                                                                                      |   |   |
|----------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|---|---|
| La couche BN attÃ©nue le dÃ©calage de covariable interne (ICS)                                                               | La couche BN facilite la tÃ¢che de lâ€™optimiseur en lui permettant dâ€™ajuster la distribution des couches cachÃ©es Ã  partir de 2 paramÃ¨tres seulement                                                          |                                                         La couche BN reparamÃ©trise le problÃ¨me dâ€™optimisation intrinsÃ¨que, le rendant plus stable et plus lisse                                                         |   |   |
| Faux : [2] a montrÃ© quâ€™en pratique, on ne distingue pas de corrÃ©lation entre cet effet et les performances dâ€™entraÃ®nement. | Peut-Ãªtre : Cette hypothÃ¨se met lâ€™accent sur lâ€™interdÃ©pendance des paramÃ¨tres du rÃ©seau entre eux, rendant difficile lâ€™optimisation des poids vers une solution optimale.  Pas de preuve solide nÃ©anmoins. | Encore trÃ¨s incertain : Leurs rÃ©sultats nâ€™ont pas encore Ã©tÃ© bousculÃ©s. Les preuves semblent encore fragiles (reposant principalement sur quelques expÃ©riences, et sur quelques Ã©lÃ©ments de dÃ©monstrations thÃ©oriques). |   |   |
|                                                                                                                            |                                                                                                                                                                                                            |                                                                                                                                                                                                                         |   |   |


De nombreuses questions demeurent, donc, et la couche BN est toujours lâ€™objet de recherches Ã  lâ€™heure oÃ¹ jâ€™Ã©cris ces lignes. Mais l'Ã©valuation de ces hypothÃ¨ses nous donnent une meilleur comprÃ©hension de la couche de normalisation par lots, nous Ã©loignant des justifications erronÃ©es que lâ€™on a eu longtemps Ã  lâ€™esprit. 

Ces questions ouvertes ne nous empÃªche cependant pas de profiter de lâ€™efficacitÃ© des couches BN dans un rÃ©seau !


### VI) En rÃ©sumÃ©

AJOUTER ENCART
La normalization par lot (ou Batch-normalization - notÃ©e BN) constitue une des plus grande avancÃ©e liÃ©es Ã  lâ€™Ã©mergence de lâ€™apprentissage profond. 

Reposant sur la succession de deux transformations linÃ©aires, cette mÃ©thode rend les entraÃ®nements de rÃ©seaux de neurones profonds (perceptrons multicouches ou rÃ©seaux convolutifs) plus rapides et plus stables. Lâ€™intÃ©rÃªt majeur de cette technique rÃ©side dans le fait quâ€™elle attÃ©nue trÃ¨s largement lâ€™impact de lâ€™interdÃ©pendance entre les poids du rÃ©seau sur les paramÃ¨tres statistiques au niveau des couches cachÃ©es. 

Ã€ lâ€™heure oÃ¹ jâ€™Ã©cris cet article, toutes les mÃ©thodes de lâ€™Ã©tat de lâ€™art exploitent massivement cette mÃ©thode, que ce soit pour lâ€™extraction de caractÃ©ristiques (EfficientNet(lien de lâ€™article)), la dÃ©tection dâ€™objets (EfficientDet (lien de lâ€™article)), la segmentation (? (lien)), â€¦ .

Si vous Ãªtes intÃ©ressÃ©s par lâ€™apprentissage profond, vous ne pourrez pas y couper !



### VII) Les questions en suspent

MÃªme si la normalisation par lots a montrÃ© son efficacitÃ© en pratique depuis des annÃ©es, ce concept est encore mal compris. Et si certains articles ont bousculÃ© la comprÃ©hension largement admise pendant des annÃ©es par la communautÃ© scientifique, les mÃ©canismes intrinsÃ¨ques qui rÃ©gissent ce concept restent trÃ¨s incertains.

En particulier, on se demande :
- Comment la normalisation par lots dâ€™aide le rÃ©seau Ã  gÃ©nÃ©raliser plus efficacement ?
- La couche BN est-elle la meilleure solution de normalization pour faciliter lâ€™optimisation ?
- Dans quelle mesure les paramÃ¨tres ğ›½ et ğ›¾ influencent le lissage du paysage dâ€™optimisation ?
- Les expÃ©rimentations montrant lâ€™effet de lissage de la couche BN sur le paysage dâ€™optimisation ont rÃ©alisÃ©es dans des conditions de court-terme ; on a regardÃ© lâ€™Ã©volution du gradient et de la fonction de coÃ»t Ã  partir dâ€™une seule itÃ©ration, testant diffÃ©rentes longueurs de pas. Au delÃ  de lâ€™impact direct que ces expÃ©riences mettent en lumiÃ¨re, quâ€™en est-il sur le long terme ? Lâ€™interdÃ©pendances des poids provoque-t-elle dâ€™autres effets remarquables sur le paysage dâ€™optimisation ?

Cette liste nâ€™est bien entendu pas exhaustive, et beaucoup de mystÃ¨res demeurent autour de la Normalisation par lots. Ã€ suivre, donc â€¦ ;)


##### Sources et rÃ©fÃ©rences


<ins>Articles :</ins>

[1] â€œNormalisation par Lots : AccÃ©lÃ©ration de lâ€™entraÃ®nement des rÃ©seaux de neurones profonds par la rÃ©duction du dÃ©calage de covariable interneâ€, lâ€™article original : [article](https://arxiv.org/abs/1502.03167) 

[2] â€œComment la normalisation par lots aide lâ€™optimisation.â€ : [article](https://arxiv.org/pdf/1805.11604.pdf) 


RÃ©seau Inception : [article](https://arxiv.org/abs/1409.4842 ) 


<ins>Liens :</ins>

Brillante prÃ©sentation de Ian Goodfellow (malgrÃ© la qualitÃ© sonore), dont le dÃ©but traite de la normalisation par lot :
https://www.youtube.com/watch?v=Xogn6veSyxA

PrÃ©sentation de lâ€™article â€œComment la normalisation par lots aide lâ€™optimisation ?â€ par lâ€™un des auteurs chez Microsoft ; lâ€™audience est incisive sur les questions, les dÃ©bats dÃ©clenchÃ©s sont passionnants :
https://www.microsoft.com/en-us/research/video/how-does-batch-normalization-help-optimization/


Ã€ propos de la moyenne mobile : [lien](https://fr.wikipedia.org/wiki/Moyenne_mobile)


ExpÃ©rimentation - BN aprÃ¨s lâ€™activation donne de meilleurs rÃ©sultats quâ€™avant : [lien](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu)


Positionnement de la BN avant ou aprÃ¨s lâ€™activation : [lien](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout)


Citation de F. Chollet Ã  propos de la place de la BN : [lien](https://github.com/keras-team/keras/issues/1802)



-----------



###

### head3
#### head4
##### head5
###### head6




Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyllâ€™s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
